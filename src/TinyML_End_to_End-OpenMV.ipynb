{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.preprocessing.image import (ImageDataGenerator, Iterator,\n",
    "                                       array_to_img, img_to_array, load_img)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import shap\n",
    "import keras\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "# sess.as_default()\n",
    "\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for physical_device in physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(physical_device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../data/CollisionData/'\n",
    "\n",
    "img_width, img_height = 96, 96 # 224, 224\n",
    "nb_train_samples = 730 \n",
    "nb_validation_samples = 181\n",
    "epochs = 25\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first': \n",
    "    input_shape = (3, img_width, img_height) \n",
    "else: \n",
    "    input_shape = (img_width, img_height, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 911 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    ) # set validation split\n",
    "\n",
    "#### train with 100% of data. See other notebooks for validation of the models\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary') # set as training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [] # can be used for multiple models, for this we are using a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny Compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_CNN = tf.keras.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=16, kernel_size=5, padding='same', activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "    \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "    \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "finetune_model_CNN._name=\"Custom_CNN\"\n",
    "\n",
    "finetune_model_CNN.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr = 1e-5, decay = 1e-5),\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(finetune_model_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Custom_CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 96, 96, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 96, 96, 16)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,258,829\n",
      "Trainable params: 1,258,727\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "finetune_model_CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Training and Testing Model: Custom_CNN\n",
      "Epoch 1/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.6801 - loss: 0.6365WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 200ms/step - accuracy: 0.6801 - loss: 0.6365\n",
      "Epoch 2/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.7177 - loss: 0.5648WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.7177 - loss: 0.5648\n",
      "Epoch 3/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.7719 - loss: 0.4866WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.7719 - loss: 0.4866\n",
      "Epoch 4/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.7778 - loss: 0.4656WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 200ms/step - accuracy: 0.7778 - loss: 0.4656\n",
      "Epoch 5/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.7816 - loss: 0.4519WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.7816 - loss: 0.4519\n",
      "Epoch 6/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8095 - loss: 0.4209WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8095 - loss: 0.4209\n",
      "Epoch 7/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8053 - loss: 0.3922WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 199ms/step - accuracy: 0.8053 - loss: 0.3922\n",
      "Epoch 8/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8220 - loss: 0.3586WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 200ms/step - accuracy: 0.8220 - loss: 0.3586\n",
      "Epoch 9/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8414 - loss: 0.3634WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8414 - loss: 0.3634\n",
      "Epoch 10/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8595 - loss: 0.3421WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8595 - loss: 0.3421\n",
      "Epoch 11/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8581 - loss: 0.3143WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 199ms/step - accuracy: 0.8581 - loss: 0.3143\n",
      "Epoch 12/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8623 - loss: 0.3134WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 198ms/step - accuracy: 0.8623 - loss: 0.3134\n",
      "Epoch 13/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8665 - loss: 0.3159WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 200ms/step - accuracy: 0.8665 - loss: 0.3159\n",
      "Epoch 14/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8957 - loss: 0.2772WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 200ms/step - accuracy: 0.8957 - loss: 0.2772\n",
      "Epoch 15/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8637 - loss: 0.2854WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8637 - loss: 0.2854\n",
      "Epoch 16/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8889 - loss: 0.2655WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8889 - loss: 0.2655\n",
      "Epoch 17/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8764 - loss: 0.2799WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 200ms/step - accuracy: 0.8764 - loss: 0.2799\n",
      "Epoch 18/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8748 - loss: 0.2765WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.8748 - loss: 0.2765\n",
      "Epoch 19/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8790 - loss: 0.2637WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8790 - loss: 0.2637\n",
      "Epoch 20/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8985 - loss: 0.2460WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.8985 - loss: 0.2460\n",
      "Epoch 21/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8957 - loss: 0.2637WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8957 - loss: 0.2637\n",
      "Epoch 22/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8846 - loss: 0.2628WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8846 - loss: 0.2628\n",
      "Epoch 23/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8915 - loss: 0.2497WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 200ms/step - accuracy: 0.8915 - loss: 0.2497\n",
      "Epoch 24/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.8832 - loss: 0.2457WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.8832 - loss: 0.2457\n",
      "Epoch 25/25\n",
      "45/45 [==============================] - ETA: 0s - accuracy: 0.9026 - loss: 0.2484WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "45/45 [==============================] - 9s 199ms/step - accuracy: 0.9026 - loss: 0.2484\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "             tf.keras.callbacks.EarlyStopping(\n",
    "                 monitor='val_loss', patience = 15,\n",
    "                 min_delta=0.001, restore_best_weights=True\n",
    "             )\n",
    "]\n",
    "\n",
    "history = []\n",
    "\n",
    "for each in models:\n",
    "    print(\"=\"*40)\n",
    "    print(\"Training and Testing Model: %s\" % str(each.name))\n",
    "    temp_history = each.fit(train_generator, \n",
    "        steps_per_epoch = nb_train_samples // batch_size, \n",
    "        epochs = epochs, \n",
    "        validation_steps = nb_validation_samples // batch_size, shuffle=True, callbacks = callbacks) \n",
    "    print(\"=\"*40)\n",
    "    history.append(temp_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  Custom_CNN\n",
      "1258829\n"
     ]
    }
   ],
   "source": [
    "for each in models:\n",
    "    print(\"Number of parameters:  %s\" % str(each.name))\n",
    "    print(each.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OPS:  Custom_CNN\n",
      "WARNING:tensorflow:From /home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "2516668\n"
     ]
    }
   ],
   "source": [
    "def get_flops(model_h5_path):\n",
    "    session = tf.compat.v1.Session()\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "        with session.as_default():\n",
    "            model = tf.keras.models.load_model(model_h5_path)\n",
    "            run_meta = tf.compat.v1.RunMetadata()\n",
    "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "            flops = tf.compat.v1.profiler.profile(graph=graph,\n",
    "                                                  run_meta=run_meta, cmd='op', options=opts)\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    return flops.total_float_ops\n",
    "\n",
    "for each in models:\n",
    "    model_file = '../my-log-dir/saved_model/' + str(each.name) + '.h5'\n",
    "    print(\"Number of OPS:  %s\" % str(each.name))\n",
    "    print(get_flops(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  Custom_CNN\n",
      "Size(mb): 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for each in models:\n",
    "    model_file = '../my-log-dir/saved_model/' + str(each.name) + '.h5'\n",
    "    print(\"Model:  %s\" % str(each.name))\n",
    "    b = os.path.getsize(model_file)\n",
    "    print (\"Size(mb): %d\" % (b/1000000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '../my-log-dir/saved_model/Custom_CNN.h5'\n",
    "model = tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:219: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "Model: \"Custom_CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_batch_no (None, 96, 96, 3)         13        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d ( (None, 96, 96, 16)        2418      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 48, 48, 16)        1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 48, 48, 16)        65        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_1 (None, 48, 48, 32)        9250      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_2 (None, 48, 48, 32)        18466     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 24, 24, 32)        1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 24, 24, 32)        129       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_3 (None, 24, 24, 64)        36930     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_4 (None, 24, 24, 64)        73794     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 12, 12, 64)        1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_flatten  (None, 9216)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense (P (None, 128)               2359426   \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout  (None, 128)               1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_1  (None, 64)                16450     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dropout_ (None, 64)                1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_2  (None, 1)                 131       \n",
      "=================================================================\n",
      "Total params: 2,517,078\n",
      "Trainable params: 1,258,727\n",
      "Non-trainable params: 1,258,351\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "end_step = np.ceil(1.0 * nb_train_samples / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                   final_sparsity=0.90,\n",
    "                                                   begin_step=0,\n",
    "                                                   end_step=end_step,\n",
    "                                                   frequency=100)\n",
    "\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 1/45 [..............................] - ETA: 0s - accuracy: 0.8125 - loss: 0.3441WARNING:tensorflow:From /home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.8234 - loss: 0.4140\n",
      "Epoch 2/25\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.8554 - loss: 0.3184\n",
      "Epoch 3/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8331 - loss: 0.3261\n",
      "Epoch 4/25\n",
      "45/45 [==============================] - 9s 203ms/step - accuracy: 0.8985 - loss: 0.2573\n",
      "Epoch 5/25\n",
      "45/45 [==============================] - 9s 203ms/step - accuracy: 0.8846 - loss: 0.2736\n",
      "Epoch 6/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8957 - loss: 0.2589\n",
      "Epoch 7/25\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.9028 - loss: 0.2280\n",
      "Epoch 8/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8999 - loss: 0.2255\n",
      "Epoch 9/25\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.9360 - loss: 0.1833\n",
      "Epoch 10/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.9333 - loss: 0.1919\n",
      "Epoch 11/25\n",
      "45/45 [==============================] - 9s 205ms/step - accuracy: 0.9458 - loss: 0.1470\n",
      "Epoch 12/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.9152 - loss: 0.1790\n",
      "Epoch 13/25\n",
      "45/45 [==============================] - 9s 204ms/step - accuracy: 0.9361 - loss: 0.1723\n",
      "Epoch 14/25\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.9194 - loss: 0.2041\n",
      "Epoch 15/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.9346 - loss: 0.1566\n",
      "Epoch 16/25\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.9249 - loss: 0.1852\n",
      "Epoch 17/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.9125 - loss: 0.2331\n",
      "Epoch 18/25\n",
      "45/45 [==============================] - 9s 203ms/step - accuracy: 0.9305 - loss: 0.1819\n",
      "Epoch 19/25\n",
      "45/45 [==============================] - 9s 203ms/step - accuracy: 0.9125 - loss: 0.1866\n",
      "Epoch 20/25\n",
      "45/45 [==============================] - 9s 203ms/step - accuracy: 0.9235 - loss: 0.1808\n",
      "Epoch 21/25\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.9040 - loss: 0.1891\n",
      "Epoch 22/25\n",
      "45/45 [==============================] - 9s 202ms/step - accuracy: 0.9264 - loss: 0.1862\n",
      "Epoch 23/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8985 - loss: 0.2048\n",
      "Epoch 24/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.8915 - loss: 0.1834\n",
      "Epoch 25/25\n",
      "45/45 [==============================] - 9s 201ms/step - accuracy: 0.9082 - loss: 0.1789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc1e04bfe10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = '../my-log-dir/'\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "  \n",
    "model_for_pruning.fit(train_generator, \n",
    "    steps_per_epoch = nb_train_samples // batch_size, \n",
    "    epochs = epochs,\n",
    "    validation_steps = nb_validation_samples // batch_size, shuffle=True, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Custom_CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 96, 96, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 96, 96, 16)        1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,258,829\n",
      "Trainable params: 1,258,727\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "\n",
    "final_model = sparsity.strip_pruning(model_for_pruning)\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization/gamma:0 -- Total:3, Zeros: 0.00%\n",
      "batch_normalization/beta:0 -- Total:3, Zeros: 0.00%\n",
      "batch_normalization/moving_mean:0 -- Total:3, Zeros: 0.00%\n",
      "batch_normalization/moving_variance:0 -- Total:3, Zeros: 0.00%\n",
      "conv2d/kernel:0 -- Total:1200, Zeros: 90.00%\n",
      "conv2d/bias:0 -- Total:16, Zeros: 0.00%\n",
      "batch_normalization_1/gamma:0 -- Total:16, Zeros: 0.00%\n",
      "batch_normalization_1/beta:0 -- Total:16, Zeros: 0.00%\n",
      "batch_normalization_1/moving_mean:0 -- Total:16, Zeros: 0.00%\n",
      "batch_normalization_1/moving_variance:0 -- Total:16, Zeros: 0.00%\n",
      "conv2d_1/kernel:0 -- Total:4608, Zeros: 90.00%\n",
      "conv2d_1/bias:0 -- Total:32, Zeros: 0.00%\n",
      "conv2d_2/kernel:0 -- Total:9216, Zeros: 90.00%\n",
      "conv2d_2/bias:0 -- Total:32, Zeros: 0.00%\n",
      "batch_normalization_2/gamma:0 -- Total:32, Zeros: 0.00%\n",
      "batch_normalization_2/beta:0 -- Total:32, Zeros: 0.00%\n",
      "batch_normalization_2/moving_mean:0 -- Total:32, Zeros: 0.00%\n",
      "batch_normalization_2/moving_variance:0 -- Total:32, Zeros: 0.00%\n",
      "conv2d_3/kernel:0 -- Total:18432, Zeros: 90.00%\n",
      "conv2d_3/bias:0 -- Total:64, Zeros: 0.00%\n",
      "conv2d_4/kernel:0 -- Total:36864, Zeros: 90.00%\n",
      "conv2d_4/bias:0 -- Total:64, Zeros: 0.00%\n",
      "dense/kernel:0 -- Total:1179648, Zeros: 90.00%\n",
      "dense/bias:0 -- Total:128, Zeros: 0.00%\n",
      "dense_1/kernel:0 -- Total:8192, Zeros: 90.00%\n",
      "dense_1/bias:0 -- Total:64, Zeros: 0.00%\n",
      "dense_2/kernel:0 -- Total:64, Zeros: 90.62%\n",
      "dense_2/bias:0 -- Total:1, Zeros: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# model = tf.keras.models.load_model(final_model)\n",
    "\n",
    "\n",
    "for i, w in enumerate(final_model.get_weights()):\n",
    "    print(\n",
    "        \"{} -- Total:{}, Zeros: {:.2f}%\".format(\n",
    "            final_model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pruned Model Size and Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving pruned model to:  ../my-log-dir/saved_model/pruned_model.pb\n",
      "WARNING:tensorflow:From /home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../my-log-dir/saved_model/pruned_model.pb/assets\n",
      "Size of the pruned model: 0.00 Mb\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "# _, new_pruned_keras_file = tempfile.mkstemp(\".h5\")\n",
    "\n",
    "new_pruned_keras_file = \"../my-log-dir/saved_model/pruned_model.pb\"\n",
    "print(\"Saving pruned model to: \", new_pruned_keras_file)\n",
    "tf.keras.models.save_model(final_model, new_pruned_keras_file, include_optimizer=False)\n",
    "print(\n",
    "    \"Size of the pruned model: %.2f Mb\"\n",
    "    % (os.path.getsize(new_pruned_keras_file) / float(2 ** 20))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post training quantization\n",
    "##### Full integer quantization. \n",
    "\n",
    "To 8-bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmposl69rti/assets\n",
      "(911, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "keras_model = tf.keras.models.load_model(new_pruned_keras_file)\n",
    "\n",
    "tflite_fullint_model_file = \"../my-log-dir/saved_model/post_fullint_quantized.tflite\"\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model('../log/saved_model/pruned_model.pb')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "\n",
    "num_calibration_steps = 1\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(num_calibration_steps):\n",
    "        # Get sample input data as a numpy array in a method of your choosing.\n",
    "        ## Ideally we should do a validation calibration but we are using all of the training data for max acc\n",
    "#         x =np.concatenate([validation_generator.next()[0] for i in range(validation_generator.__len__())])\n",
    "        x =np.concatenate([train_generator.next()[0] for i in range(train_generator.__len__())])\n",
    "        print(x.shape)\n",
    "        yield [x]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_fullint_model_file, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Files\n",
    "\n",
    "`pruned_model.pb` - Pruned model file (file size is not going to reduce but the sparsity is incorporated)\n",
    "\n",
    "`post_fullint_quantized.tflite` - Integer 8-bit quantized model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized Model to OpenMV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the MicroPython code that uses the `post_fullint_quantized.tflite` file to perform inferencing on the device (OpenMV Cam H7+).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-53b59601707d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# import what we need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#setup LEDs and set into known off state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pyb'"
     ]
    }
   ],
   "source": [
    "# %load CollisionCam.py\n",
    "# Dune Collision Camera\n",
    "# written by EB Goldstein and SD Mohanty\n",
    "# started 10/2020\n",
    "# last revision 4/2021\n",
    "\n",
    "# import what we need\n",
    "import pyb, sensor, image, time, os, tf, random\n",
    "\n",
    "#setup LEDs and set into known off state\n",
    "red_led = pyb.LED(1)\n",
    "green_led = pyb.LED(2)\n",
    "red_led.off()\n",
    "green_led.off()\n",
    "\n",
    "#red light during setup\n",
    "red_led.on()\n",
    "\n",
    "# get sensor set up\n",
    "sensor.reset()                         # Reset & initialize sensor\n",
    "sensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB\n",
    "sensor.set_framesize((sensor.QVGA))      # Set frame size to QVGA (320x240)\n",
    "sensor.set_windowing((240,240))       # Set window to 240x240\n",
    "sensor.skip_frames(time=2000)          # Let the camera adjust.\n",
    "\n",
    "#Load the TFlite model and the labels\n",
    "net = tf.load('/post_quantized_full_int.tflite', load_to_fb=True)\n",
    "labels = ['collision', 'no collision']\n",
    "\n",
    "#turn red off when model is loaded\n",
    "red_led.off()\n",
    "\n",
    "#MAIN LOOP\n",
    "\n",
    "# loop needs to do a few things:\n",
    "# x-take a picture\n",
    "# x-record the picture on sd card\n",
    "# x-do the inference\n",
    "# x-record the inference in a db w/ rand as name\n",
    "# x-blink LED for field debugging\n",
    "# x-delay\n",
    "\n",
    "while(True):\n",
    "\n",
    "    #toggle LED for visual indication that script is running\n",
    "    green_led.toggle()\n",
    "\n",
    "    #get the image/take the picture\n",
    "    img = sensor.snapshot()\n",
    "\n",
    "    #Do the classification and get the object returned by the inference.\n",
    "    TF_objs = net.classify(img)\n",
    "\n",
    "    #The object has a output, which is a list of classifcation scores\n",
    "    #for each of the output channels. this model only has 1 (Collision).\n",
    "    #So now we extract that float value and print to the serial terminal.\n",
    "\n",
    "    collision_score = TF_objs[0].output()[0]\n",
    "    print(\"Collision = %f\" % collision_score)\n",
    "\n",
    "    #we don;t have an RTC attached now, so we save the images and the\n",
    "    #collision scores with names according to a random bit stream.\n",
    "\n",
    "    #generate random bits for stream of names\n",
    "    rand_label = str(random.getrandbits(30))\n",
    "\n",
    "    #save image on camera (bit as name)\n",
    "    img.save(\"./imgs/\" + rand_label + \".jpg\")\n",
    "\n",
    "    #save inference (bit as name, and score) to a file\n",
    "    with open(\"./inference.txt\", 'a') as file:\n",
    "        file.write(rand_label + \",\" + str(collision_score) + \"\\n\")\n",
    "\n",
    "    #wait some number of milliseconds\n",
    "    pyb.delay(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3deep",
   "language": "python",
   "name": "python3deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
